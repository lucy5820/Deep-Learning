{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e312323f73714e2991f5d1eda8a4bc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_222a6a98325b4d66a220ac642d7c6f1a",
              "IPY_MODEL_cd394e007a184f558869435298a19dfc",
              "IPY_MODEL_f73237dacacd44a08408667518307029"
            ],
            "layout": "IPY_MODEL_84be4f0a34d04a1da70b0c25b4607293"
          }
        },
        "222a6a98325b4d66a220ac642d7c6f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14f014f8f4624378b1af7142ff6f1963",
            "placeholder": "​",
            "style": "IPY_MODEL_e9e1f1b7badb421380a792cd0b30c216",
            "value": "100%"
          }
        },
        "cd394e007a184f558869435298a19dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb2fd1a3d673428db9e8339401473b51",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61332e38ac1947b6894681863af54225",
            "value": 46830571
          }
        },
        "f73237dacacd44a08408667518307029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_985e6d4702e04bcc8e7dd67cbffd96cb",
            "placeholder": "​",
            "style": "IPY_MODEL_3e9ffd1b52524f49bef80e94c4409d34",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 192MB/s]"
          }
        },
        "84be4f0a34d04a1da70b0c25b4607293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f014f8f4624378b1af7142ff6f1963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e1f1b7badb421380a792cd0b30c216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb2fd1a3d673428db9e8339401473b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61332e38ac1947b6894681863af54225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "985e6d4702e04bcc8e7dd67cbffd96cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e9ffd1b52524f49bef80e94c4409d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfOXmvzADfpr"
      },
      "source": [
        "# Lab 3: Generating Data\n",
        "\n",
        "Based on assignments by Lisa Zhang and Jimmy Ba.\n",
        "\n",
        "In this lab, you will build models to perform image colourization. That is, given a greyscale image, we wish to predict the colour at each pixel. Image colourization is a difficult problem for many reasons, one of which being that it is ill-posed: for a single greyscale image, there can be multiple, equally valid colourings.\n",
        "\n",
        "To keep the training time manageable we will use the CIFAR-10 data set, which consists of images of size 32x32 pixels. For most of the questions we will use a subset of the dataset. The data loading script is included with the notebooks, and should download automatically the first time it is loaded. \n",
        "\n",
        "We will be starting with a convolutional autoencoder and tweaking it along the way to improve our perforamnce. Then as a second part of the assignment we will compare the autoencoder approach to conditional generative adversarial networks (cGANs).\n",
        "\n",
        "In the process, you are expected to learn to:\n",
        "\n",
        "1. Clean and process the dataset and create greyscale images.\n",
        "2. Implement and modify an autoencoder architecture.\n",
        "3. Tune the hyperparameters of an autoencoder.\n",
        "4. Implement skip connections and other techniques to improve performance.\n",
        "5. Implement a cGAN and compare with an autoencoder.\n",
        "6. Improve on the cGAN by trying one of several techniques to enhance training.\n",
        "\n",
        "\n",
        "### What to submit\n",
        "\n",
        "Submit an HTML file containing all your code, outputs, and write-up\n",
        "from parts A and B. You can produce a HTML file directly from Google Colab. The Colab instructions are provided at the end of this document.\n",
        "\n",
        "**Do not submit any other files produced by your code.**\n",
        "\n",
        "Include a link to your colab file in your submission.\n",
        "\n",
        "Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbnrp2ig1pps"
      },
      "source": [
        "## Colab Link\n",
        "\n",
        "Include a link to your Colab file here. If you would like the TA to look at your\n",
        "Colab file in case your solutions are cut off, **please make sure that your Colab\n",
        "file is publicly accessible at the time of submission**.\n",
        "\n",
        "Colab Link:https://colab.research.google.com/drive/1rQYZc-gJ81uY6W_hFZN6v8ws1myQ3pJg?authuser=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFMdtipUPNdu"
      },
      "source": [
        "# PART A - Autoencoder [20 pt]\n",
        "\n",
        "In this part we will construct and compare different autoencoder models for the image colourization task."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARWjhFeID_Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "#### Helper code\n",
        "\n",
        "Provided are some helper functions for loading and preparing the data. Note that you will need to use the Colab GPU for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTF1TQObE6DG"
      },
      "source": [
        "\"\"\"\n",
        "Colourization of CIFAR-10 Horses via classification.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import scipy.misc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE_dtMgIh0SJ",
        "outputId": "2d733ea4-6250-47ae-cedb-52c1f694108e"
      },
      "source": [
        "######################################################################\n",
        "# Setup working directory\n",
        "######################################################################\n",
        "%mkdir -p /content/a3/\n",
        "%cd /content/a3\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/a3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piDmAsqFG0gU"
      },
      "source": [
        "######################################################################\n",
        "# Helper functions for loading data\n",
        "######################################################################\n",
        "# adapted from\n",
        "# https://github.com/fchollet/keras/blob/master/keras/datasets/cifar10.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "def get_file(fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(\"File path: %s\" % fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "def load_batch(fpath, label_key=\"labels\"):\n",
        "    \"\"\"Internal utility for parsing CIFAR data.\n",
        "    # Arguments\n",
        "        fpath: path the file to parse.\n",
        "        label_key: key for label data in the retrieve\n",
        "            dictionary.\n",
        "    # Returns\n",
        "        A tuple `(data, labels)`.\n",
        "    \"\"\"\n",
        "    f = open(fpath, \"rb\")\n",
        "    if sys.version_info < (3,):\n",
        "        d = pickle.load(f)\n",
        "    else:\n",
        "        d = pickle.load(f, encoding=\"bytes\")\n",
        "        # decode utf8\n",
        "        d_decoded = {}\n",
        "        for k, v in d.items():\n",
        "            d_decoded[k.decode(\"utf8\")] = v\n",
        "        d = d_decoded\n",
        "    f.close()\n",
        "    data = d[\"data\"]\n",
        "    labels = d[label_key]\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def load_cifar10(transpose=False):\n",
        "    \"\"\"Loads CIFAR10 dataset.\n",
        "    # Returns\n",
        "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
        "    \"\"\"\n",
        "    dirname = \"cifar-10-batches-py\"\n",
        "    origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    path = get_file(dirname, origin=origin, untar=True)\n",
        "\n",
        "    num_train_samples = 50000\n",
        "\n",
        "    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype=\"uint8\")\n",
        "    y_train = np.zeros((num_train_samples,), dtype=\"uint8\")\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        fpath = os.path.join(path, \"data_batch_\" + str(i))\n",
        "        data, labels = load_batch(fpath)\n",
        "        x_train[(i - 1) * 10000 : i * 10000, :, :, :] = data\n",
        "        y_train[(i - 1) * 10000 : i * 10000] = labels\n",
        "\n",
        "    fpath = os.path.join(path, \"test_batch\")\n",
        "    x_test, y_test = load_batch(fpath)\n",
        "\n",
        "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
        "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
        "\n",
        "    if transpose:\n",
        "        x_train = x_train.transpose(0, 2, 3, 1)\n",
        "        x_test = x_test.transpose(0, 2, 3, 1)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7fti3cryStt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50861f3-05a5-4f81-c1b1-0fc9811385ff"
      },
      "source": [
        "# Download CIFAR dataset\n",
        "m = load_cifar10()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File path: data/cifar-10-batches-py.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2TWonhyn0FK"
      },
      "source": [
        "## Part 1. Data Preparation [7 pt]\n",
        "\n",
        "To start off run the above code to load the CIFAR dataset and then work through the following questions/tasks. \n",
        "\n",
        "### Part (a) [1pt]\n",
        "Verify that the dataset has loaded correctly. How many samples do we have? How is the data organized?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6bV7uvsh9Sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2999d3b-7d8c-4bc3-9705-92a863bc2171"
      },
      "source": [
        "# code to examine the dataset\n",
        "print(\"there are\", len(m[0][0]),\"samples in training dataset\") # x-train\n",
        "print(\"there are\", len(m[1][0]),\"samples in testing dataset\") # x-test\n",
        "print(\"in total, there are\",len(m[0][0])+len(m[1][0]), \"samples.\") # total samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 50000 samples in training dataset\n",
            "there are 10000 samples in testing dataset\n",
            "in total, there are 60000 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "83% of the total samples are training samples and 17% of them are testing samples. each sample is a 3 dimensions(RGB) 32*32 colorful image.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The dataset consists 10 classes, with 6000 images per class."
      ],
      "metadata": {
        "id": "4hlRgcLhDexR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDqVs87xpA40"
      },
      "source": [
        "### Part (b) [2pt]\n",
        "Preprocess the data to select only images of horses. Learning to generate only hourse images will make our task easier. Your function will also convert the colour images to greyscale to create our input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-900ROTMlSPd"
      },
      "source": [
        "# select a single category.\n",
        "HORSE_CATEGORY = 7\n",
        "\n",
        "# convert colour images into greyscale\n",
        "def process(xs, ys, max_pixel=256.0, downsize_input=False):\n",
        "    \"\"\"\n",
        "    Pre-process CIFAR10 images by taking only the horse category,\n",
        "    shuffling, and have colour values be bound between 0 and 1\n",
        "\n",
        "    Args:\n",
        "      xs: the colour RGB pixel values\n",
        "      ys: the category labels\n",
        "      max_pixel: maximum pixel value in the original data\n",
        "    Returns:\n",
        "      xs: value normalized and shuffled colour images\n",
        "      grey: greyscale images, also normalized so values are between 0 and 1\n",
        "    \"\"\"\n",
        "    xs = xs / max_pixel\n",
        "    xs = xs[np.where(ys == HORSE_CATEGORY)[0], :, :, :]\n",
        "    npr.shuffle(xs)\n",
        "\n",
        "    grey = np.mean(xs, axis=1, keepdims=True)\n",
        "\n",
        "    if downsize_input:\n",
        "        downsize_module = nn.Sequential(\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "        )\n",
        "        xs_downsized = downsize_module.forward(torch.from_numpy(xs).float())\n",
        "        xs_downsized = xs_downsized.data.numpy()\n",
        "        return (xs, xs_downsized)\n",
        "    else:\n",
        "        return (xs, grey)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHSeX3ADsQu5"
      },
      "source": [
        "### Part (c) [2pt]\n",
        "Create a dataloader (or function) to batch the samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kU_rnNssXwU"
      },
      "source": [
        "# dataloader for batching samples\n",
        "\n",
        "def get_batch(x, y, batch_size):\n",
        "    \"\"\"\n",
        "    Generated that yields batches of data\n",
        "\n",
        "    Args:\n",
        "      x: input values\n",
        "      y: output values\n",
        "      batch_size: size of each batch\n",
        "    Yields:\n",
        "      batch_x: a batch of inputs of size at most batch_size\n",
        "      batch_y: a batch of outputs of size at most batch_size\n",
        "    \"\"\"\n",
        "    N = np.shape(x)[0]\n",
        "    assert N == np.shape(y)[0]\n",
        "    for i in range(0, N, batch_size):\n",
        "        batch_x = x[i : i + batch_size, :, :, :]\n",
        "        batch_y = y[i : i + batch_size, :, :, :]\n",
        "        yield (batch_x, batch_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjPTh0sjsgkV"
      },
      "source": [
        "### Part (e) [2pt]\n",
        "Verify and visualize that we are able to generate different batches of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7zubylhiipE",
        "outputId": "9d8f07ff-ee64-43fa-b5d1-d2504dd03cb1"
      },
      "source": [
        "# code to load different batches of horse dataset\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "print(\"Transforming data...\")\n",
        "train_rgb, train_grey = process(x_train, y_train)\n",
        "test_rgb, test_grey = process(x_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "File path: data/cifar-10-batches-py.tar.gz\n",
            "Transforming data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wek0NfrHjNWE",
        "outputId": "c9da1480-7ff3-42b9-ee2d-3097108a99a0"
      },
      "source": [
        "# shape of data and labels before selection\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3, 32, 32) (50000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qpj_1-yjfM-",
        "outputId": "2077e87e-a731-47f5-92de-8a06da804e50"
      },
      "source": [
        "# shape of training data\n",
        "print('Training Data: ', train_rgb.shape, train_grey.shape)\n",
        "# shape of testing data\n",
        "print('Testing Data: ', test_rgb.shape, test_grey.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:  (5000, 3, 32, 32) (5000, 1, 32, 32)\n",
            "Testing Data:  (1000, 3, 32, 32) (1000, 1, 32, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHBOzMJPpNbq"
      },
      "source": [
        "Load Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfGh9pZOpP3r",
        "outputId": "86bf95e2-c806-4057-ac2f-fe44a619dbc1"
      },
      "source": [
        "# obtain batches of images\n",
        "# batch size = 10, set first 10 images as our first batch\n",
        "xs, ys = next(iter(get_batch(train_grey, train_rgb, 10)))\n",
        "print(xs.shape, ys.shape)\n",
        "\n",
        "# batch size = 10, set the following 10 images as our second batch\n",
        "xs1, ys1 = next(iter(get_batch(train_grey[10:, :, :, :], train_rgb[10:, :, :, :], 10)))\n",
        "print(xs1.shape, ys1.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1, 32, 32) (10, 3, 32, 32)\n",
            "(10, 1, 32, 32) (10, 3, 32, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUg_-oB_o9Wf"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bGPAI4Go_q_"
      },
      "source": [
        "# visualize 5 train/test images\n",
        "# Verify and visualize that we are able to generate different batches of data.\n",
        "\n",
        "# plot the train images in the first batch (batch size = 10)\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(5):\n",
        "  ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) \n",
        "  plt.imshow(np.transpose(ys[idx], (1, 2, 0)))\n",
        "\n",
        "# plot the train images (grayscale) in the first batch (batch size = 10)\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(5):\n",
        "  ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) \n",
        "  plt.imshow(xs[idx][0, :, :], cmap = 'gray')\n",
        "\n",
        "# plot the train images in the second batch (batch size = 10)\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(5):\n",
        "  ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) \n",
        "  plt.imshow(np.transpose(ys1[idx], (1, 2, 0)))\n",
        "\n",
        "# plot the train images (grayscale) in the second batch (batch size = 10)\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(5):\n",
        "  ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) \n",
        "  plt.imshow(xs1[idx][0, :, :], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAqGXV0iK1G9"
      },
      "source": [
        "## Part 2. Colourization as Regression [5 pt]\n",
        "\n",
        "There are many ways to frame the problem of image colourization as a machine learning problem. One naive approach is to frame it as a regression problem, where we build a model to predict the RGB intensities at each pixel given the greyscale input. In this case, the outputs are continuous, and so squared error can be used to train the model.\n",
        "\n",
        "In this section, you will get familar with training neural networks using cloud GPUs. Run the helper code and answer the questions that follow.\n",
        "\n",
        "#### Helper Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hNqfK1AktAE"
      },
      "source": [
        "Regression Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOG_sFloK9gs"
      },
      "source": [
        "class RegressionCNN(nn.Module):\n",
        "    def __init__(self, kernel, num_filters):\n",
        "        # first call parent's initialization function\n",
        "        super().__init__()\n",
        "        padding = kernel // 2\n",
        "\n",
        "        self.downconv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),)\n",
        "        self.downconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),)\n",
        "\n",
        "        self.rfconv = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.upconv1 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),)\n",
        "        self.upconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, 3, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),)\n",
        "        self.finalconv = nn.Conv2d(3, 3, kernel_size=kernel, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.downconv1(x)\n",
        "        out = self.downconv2(out)\n",
        "        out = self.rfconv(out)\n",
        "        out = self.upconv1(out)\n",
        "        out = self.upconv2(out)\n",
        "        out = self.finalconv(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nXIHUDdkbe_"
      },
      "source": [
        "Training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPJG13lnka2w"
      },
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def get_torch_vars(xs, ys, gpu=False):\n",
        "    \"\"\"\n",
        "    Helper function to convert numpy arrays to pytorch tensors.\n",
        "    If GPU is used, move the tensors to GPU.\n",
        "\n",
        "    Args:\n",
        "      xs (float numpy tenosor): greyscale input\n",
        "      ys (int numpy tenosor): rgb as labels\n",
        "      gpu (bool): whether to move pytorch tensor to GPU\n",
        "    Returns:\n",
        "      Variable(xs), Variable(ys)\n",
        "    \"\"\"\n",
        "    xs = torch.from_numpy(xs).float()\n",
        "    ys = torch.from_numpy(ys).float()\n",
        "    if gpu:\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "    return Variable(xs), Variable(ys)\n",
        "\n",
        "def train(args, gen=None):\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if gen is None:\n",
        "        Net = globals()[args.model]\n",
        "        gen = Net(args.kernel, args.num_filters)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(gen.parameters(), lr=args.learn_rate)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        gen.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        gen.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = gen(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.data.item())\n",
        "\n",
        "        # validation loss\n",
        "        for k, (xs_t, ys_t) in enumerate(get_batch(test_grey, test_rgb, args.batch_size)):\n",
        "            images_t, labels_t = get_torch_vars(xs_t, ys_t, args.gpu)\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs_t = gen(images_t)\n",
        "\n",
        "            loss_t = criterion(outputs_t, labels_t)\n",
        "            loss_t.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        print(epoch, \"training loss:\", loss.cpu().detach(), \"validation loss:\", loss_t.cpu().detach())\n",
        "        if args.plot:\n",
        "          visual(images, labels, outputs, args.gpu, 1)\n",
        "    \n",
        "    return gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppqKboqz-zX-"
      },
      "source": [
        "Training visualization code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbL_Vlao-yau"
      },
      "source": [
        "# visualize 5 train/test images\n",
        "def visual(img_grey, img_real, img_fake, gpu = 0, flag_torch = 0):\n",
        "\n",
        "  if gpu:\n",
        "    img_grey = img_grey.cpu().detach()\n",
        "    img_real = img_real.cpu().detach()\n",
        "    img_fake = img_fake.cpu().detach()\n",
        "\n",
        "  if flag_torch:\n",
        "    img_grey = img_grey.numpy()\n",
        "    img_real = img_real.numpy()\n",
        "    img_fake = img_fake.numpy()\n",
        "\n",
        "  if flag_torch == 2:\n",
        "    img_real = np.transpose(img_real[:, :, :, :, :], [0, 4, 2, 3, 1]).squeeze()\n",
        "    img_fake = np.transpose(img_fake[:, :, :, :, :], [0, 4, 2, 3, 1]).squeeze()\n",
        "\n",
        "  #correct image structure\n",
        "  img_grey = np.transpose(img_grey[:5, :, :, :], [0, 2, 3, 1]).squeeze()\n",
        "  img_real = np.transpose(img_real[:5, :, :, :], [0, 2, 3, 1])\n",
        "  img_fake = np.transpose(img_fake[:5, :, :, :], [0, 2, 3, 1])\n",
        "\n",
        "  for i in range(5):\n",
        "      ax = plt.subplot(3, 5, i + 1)\n",
        "      ax.imshow(img_grey[i], cmap='gray')\n",
        "      ax.axis(\"off\")\n",
        "      ax = plt.subplot(3, 5, i + 1 + 5)\n",
        "      ax.imshow(img_real[i])\n",
        "      ax.axis(\"off\")\n",
        "      ax = plt.subplot(3, 5, i + 1 + 10)\n",
        "      ax.imshow(img_fake[i])\n",
        "      ax.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTZWiuxMjQTB"
      },
      "source": [
        "Main training loop for regression CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZHc_eStGAQz"
      },
      "source": [
        "#Main training loop for CNN\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"RegressionCNN\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KCy7KexsLgl"
      },
      "source": [
        "### Part (a) [1 pt]\n",
        "Describe the model RegressionCNN. How many convolution layers does it have? What are the filter sizes and number of filters at each layer? Construct a table or draw a diagram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Zx_yc-f98q"
      },
      "source": [
        "Answer: in model RegressionCNN, there are 6 convolution layers. the kernel size is 3\\*3, thus the filter size is also 3*3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# initialize list of lists\n",
        "data = [[1, 1, 32], [2, 32, 64], [3, 64, 64],[4, 64, 32],[5, 32, 3], [6, 3, 3]]\n",
        "  \n",
        "# Create the pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['convolution layer', 'number of filters', 'output channels(number of kernels)'])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "59E30oNAiTKS",
        "outputId": "b7d3916d-6917-499d-b29d-1475d00d1ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   convolution layer  number of filters  output channels(number of kernels)\n",
              "0                  1                  1                                  32\n",
              "1                  2                 32                                  64\n",
              "2                  3                 64                                  64\n",
              "3                  4                 64                                  32\n",
              "4                  5                 32                                   3\n",
              "5                  6                  3                                   3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0c04349-8373-4590-8bbd-8dd7dd10062d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>convolution layer</th>\n",
              "      <th>number of filters</th>\n",
              "      <th>output channels(number of kernels)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0c04349-8373-4590-8bbd-8dd7dd10062d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d0c04349-8373-4590-8bbd-8dd7dd10062d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d0c04349-8373-4590-8bbd-8dd7dd10062d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXTM_iITgED9"
      },
      "source": [
        "### Part (b) [1 pt]\n",
        "Run the regression training code (should run without errors). This will generate some images. How many epochs are we training the CNN model in the given setting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-JJ2KD4gED-"
      },
      "source": [
        "Answer: there are 25 epochs in the given setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5hSmTLxgEWG"
      },
      "source": [
        "### Part (c) [3 pt]\n",
        "Re-train a couple of new models using a different number of training epochs. You may train each new models in a new code cell by copying and modifying the code from the last notebook cell. Comment on how the results (output images, training loss) change as we increase or decrease the number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fTXWbe4gEWH"
      },
      "source": [
        "Answer: \n",
        "\n",
        "when we increase the number of epochs, we have a smaller final training loss, which indicates the generated image is getting more accurate. Besides, the final generated images have a higher resolution.\n",
        "\n",
        "Similarly, when we decrease the number of epochs, the final training loss would be greater, which shows the images have a greater probability to be considered as 'fake'. Besides, the final generated images have a lower resolution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Main training loop for CNN - with 10 epochs\n",
        "args = AttrDict()\n",
        "args_dict1 = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"RegressionCNN\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 10,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict1)\n",
        "cnn = train(args)"
      ],
      "metadata": {
        "id": "U8O4KgNzk1uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Main training loop for CNN - with 40 epochs\n",
        "args = AttrDict()\n",
        "args_dict1 = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"RegressionCNN\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 40,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict1)\n",
        "cnn = train(args)"
      ],
      "metadata": {
        "id": "IOw94-7_mOUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb248qFXhq1m"
      },
      "source": [
        "## Part 3. Skip Connections [8 pt]\n",
        "A skip connection in a neural network is a connection which skips one or more layer and connects to a later layer. We will introduce skip connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "babFMNf_h9UM"
      },
      "source": [
        "### Part (a) [4 pt]\n",
        "Add a skip connection from the first layer to the last, second layer to the second last, etc.\n",
        "That is, the final convolution should have both the output of the previous layer and the initial greyscale input as input. This type of skip-connection is introduced by [3], and is called a \"UNet\". Following the CNN class that you have completed, complete the __init__ and forward methods of the UNet class.\n",
        "Hint: You will need to use the function torch.cat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsipREOi1r7F"
      },
      "source": [
        "#complete the code\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        #contracting path\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.downconv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, num_filters, kernel_size=kernel,stride = 2, padding=padding), #32*16*16\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()) \n",
        "        \n",
        "        self.downconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, stride = 2, padding=padding), #64*4*4\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU()) #64*2*2\n",
        "\n",
        "        #bottommost\n",
        "        self.rfconv = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU()) #64*2*2\n",
        "        \n",
        "        #expansive path\n",
        "        self.upconv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(num_filters*2, num_filters*2, kernel_size=kernel, stride = 2, padding=padding, output_padding = 1), \n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU()) #64*4*4\n",
        "\n",
        "        self.upconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*4, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU(),  #64*4*4\n",
        "\n",
        "            nn.ConvTranspose2d(num_filters*2, num_filters, kernel_size=kernel, stride = 2, padding=padding, output_padding = 1), \n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),) #32*16*16\n",
        "\n",
        "        self.upconv3 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),  #32*16*16\n",
        "\n",
        "            nn.ConvTranspose2d(num_filters, 1, kernel_size=kernel, stride = 2, padding=padding, output_padding = 1), \n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.ReLU()) #1*32*32\n",
        "\n",
        "        self.finalconv = nn.Conv2d(2, 3, kernel_size=kernel, padding=padding)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.downconv1(x) #32*16*16\n",
        "        conv2 = self.downconv2(self.pool(conv1)) #after pooling: 32*8*8; conv2: 64*4*4\n",
        "        center = self.rfconv(self.pool(conv2)) # after pooling: 64*2*2; center: 64*2*2\n",
        "\n",
        "        upconv1 = self.upconv1(center) #upconv1:64*4*4\n",
        "        upconv2 = self.upconv2(torch.cat([upconv1, conv2], 1)) #after concatenate: 128*4*4; upconv2: 32*16*16\n",
        "        upconv3 = self.upconv3(torch.cat([upconv2, conv1], 1)) #after concatenate: 64*16*16; upconv2: 1*32*32\n",
        "\n",
        "        out = self.finalconv(torch.cat([upconv3, x], 1)) #input:2*32*32; output: 3*32*32\n",
        "        return out\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VzFtk3hh9UN"
      },
      "source": [
        "### Part (b) [2 pt]\n",
        "Train the \"UNet\" model for the same amount of epochs as the previous CNN and plot the training curve using a batch size of 100. How does the result compare to the previous model? Did skip connections improve the validation loss and accuracy? Did the skip connections improve the output qualitatively? How? Give at least two reasons why skip connections might improve the performance of our CNN models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-vGs7qHndmY"
      },
      "source": [
        "# Main training loop for UNet\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compared with our previous model, the generated images from Unet model has a higher resolution and make the image more accurate (has more details). The Unet model has also a lower validation loss, which means the accuracy of Unet model is higher than previous model and Unet has a greater generality.\n",
        "\n",
        "The reasons are the model reuses features by concatenating them to new layer, so more information is retian from encoding layers and allows recover spatial information lost during downsampling.\n",
        "\n",
        "Besides, skip connections uninterrupted gradient flow from the first layer to the last layer, which tackles the vanishing gradient problem."
      ],
      "metadata": {
        "id": "BftUVWJ7H8Ao"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fl2PcliJTd"
      },
      "source": [
        "### Part (c) [2 pt]\n",
        "Re-train a few more \"UNet\" models using different mini batch sizes with a fixed number of epochs. Describe the effect of batch sizes on the training/validation loss, and the final image output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWF83JjpiJTd"
      },
      "source": [
        "# complete the code\n",
        "# batch size = 160\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 160,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complete the code\n",
        "# batch size = 64\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ],
      "metadata": {
        "id": "dyIC_LSjQ1-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complete the code\n",
        "# batch size = 32\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 32,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ],
      "metadata": {
        "id": "8FEUa6FTR6eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared these models, the training loss is 0.0068 when batch size is 160; 0.0.0052 when batch size is 100; 0.0.0048 when batch size is 64, and 0.0016 when batch size is 32;. Hence, the loss is getting smaller when we decrease our batch size.\n",
        "Compared the generated images from the model, the images with the least batch size(batch size=32) have a highest resolution and contain most details. Hence the overall quality of images with model batch size 32 is the highest."
      ],
      "metadata": {
        "id": "tAq2JE4RSaOk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCjRpOlEuuJj"
      },
      "source": [
        "# PART B - Conditional GAN [30 pt]\n",
        "\n",
        "In this second half of the assignment we will construct a conditional generative adversarial network for our image colourization task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgohSYXiWnD"
      },
      "source": [
        "## Part 1. Conditional GAN [15 pt]\n",
        "\n",
        "To start we will be modifying the previous sample code to construct and train a conditional GAN. We will exploring the different architectures to identify and select our best image colourization model.\n",
        "\n",
        "Note: This second half of the assignment should be started after the lecture on generative adversarial networks (GANs). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-TzCQBe_6Uk"
      },
      "source": [
        "### Part (a) [3 pt]\n",
        "Modify the provided training code to implement a generator. Then test to verify it works on the desired input (Hint: you can reuse some of your earlier autoencoder models here to act as a generator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRB4eJQIFFAD"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        #contracting path\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.downconv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, num_filters, kernel_size=kernel,stride = 2, padding=padding), #16*16*16\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.LeakyReLU(0.2)) \n",
        "        \n",
        "        self.downconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, stride = 2, padding=padding), #32*4*4\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.LeakyReLU(0.2)) #32*2*2\n",
        "\n",
        "        #bottommost\n",
        "        self.rfconv = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.LeakyReLU(0.2)) #32*2*2\n",
        "        \n",
        "        #expansive path\n",
        "        self.upconv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(num_filters*2, num_filters*2, kernel_size=kernel, stride = 2, padding=padding, output_padding = 1), \n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.LeakyReLU(0.2)) #32*4*4\n",
        "\n",
        "        self.upconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*4, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.LeakyReLU(0.2),  #32*4*4\n",
        "\n",
        "            nn.ConvTranspose2d(num_filters*2, num_filters, kernel_size=kernel, stride = 2, padding=padding, output_padding = 1), \n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Upsample(scale_factor=2),) #16*16*16\n",
        "\n",
        "        self.upconv3 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.LeakyReLU(0.2),  #16*16*16\n",
        "\n",
        "            nn.ConvTranspose2d(num_filters, 1, kernel_size=kernel, stride = 2, padding=padding, output_padding = 1), \n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.LeakyReLU(0.2)) #1*32*32\n",
        "\n",
        "        self.finalconv = nn.Conv2d(2, 3, kernel_size=kernel, padding=padding)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.downconv1(x) #16*16*16\n",
        "        conv2 = self.downconv2(self.pool(conv1)) #after pooling: 16*8*8; conv2: 32*4*4\n",
        "        center = self.rfconv(self.pool(conv2)) # after pooling: 32*2*2; center: 32*2*2\n",
        "\n",
        "        upconv1 = self.upconv1(center) #upconv1:64*4*4\n",
        "        upconv2 = self.upconv2(torch.cat([upconv1, conv2], 1)) #after concatenate: 64*4*4; upconv2: 16*16*16\n",
        "        upconv3 = self.upconv3(torch.cat([upconv2, conv1], 1)) #after concatenate: 32*16*16; upconv2: 1*32*32\n",
        "\n",
        "        out = self.finalconv(torch.cat([upconv3, x], 1)) #input:2*32*32; output: 3*32*32\n",
        "        out = F.tanh(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator(args, gen=None):\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if gen is None:\n",
        "        Net = globals()[args.model]\n",
        "        gen = Net(args.kernel, args.num_filters)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(gen.parameters(), lr=args.learn_rate)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        gen.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        gen.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = gen(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.data.item())\n",
        "        \n",
        "        print(epoch, loss.cpu().detach())\n",
        "        if args.plot:\n",
        "          visual(images, labels, outputs, args.gpu, 1)\n",
        "    \n",
        "    return gen"
      ],
      "metadata": {
        "id": "Czer-f1yhIg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"Generator\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 1,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "generator = train_generator(args)\n",
        "\n",
        "# if this model could generate images, then it shows our generator works."
      ],
      "metadata": {
        "id": "hQ8IKLxghc0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o4lkOgF_3A2"
      },
      "source": [
        "### Part (b) [3 pt]\n",
        "Modify the provided training code to implement a discriminator. Then test to verify it works on the desired input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHvxX5jrF0Pw"
      },
      "source": [
        "# discriminator code\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        self.downconv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, num_filters, kernel_size=kernel,stride = 2, padding=padding), #32*16*16\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.LeakyReLU(0.2)) \n",
        "        \n",
        "        self.downconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, stride = 2, padding=padding), #64*8*8\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.LeakyReLU(0.2)) \n",
        "        \n",
        "        self.fc1 = nn.Linear(64*8*8, 15)\n",
        "        self.fc2 = nn.Linear(15, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "    \n",
        "    def forward(self, x, img_greyscale):\n",
        "        x = torch.cat([x, img_greyscale], 1)\n",
        "        conv1 = self.downconv1(x)\n",
        "        conv2 = self.downconv2(conv1)\n",
        "\n",
        "        out = conv2.view(-1, 64*8*8)\n",
        "\n",
        "        out = self.leakyrelu(self.fc1(out))\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "        out = out.squeeze()\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaw5C6w9Wrlx"
      },
      "source": [
        "# test discriminator architecture\n",
        "\n",
        "generator = Generator(3,32).cuda()\n",
        "\n",
        "\n",
        "def train_discriminator(args, gen=None):\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if gen is None:\n",
        "        Net = globals()[args.model]\n",
        "        gen = Net(args.kernel, args.num_filters)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(gen.parameters(), lr=args.learn_rate)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        gen.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        gen.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            generator.train()\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "            \n",
        "            real_validity = gen(labels, images)\n",
        "            fake_images = generator(images)\n",
        "\n",
        "            #print the output shape when training real image \n",
        "            #if it could generate the output, then it has the ability to classify whether the image is real or fake.\n",
        "            #Thus it is possible to compute overall loss and train the discriminator\n",
        "            \n",
        "            print(real_validity.shape) \n",
        "            \n",
        "            #print the output shape when training fake image \n",
        "            #if it could generate the output, then it has the ability to classify whether the image is real or fake.\n",
        "            #Thus it is possible to compute overall loss and train the discriminator\n",
        "\n",
        "            fake_validity = gen(fake_images, images)\n",
        "            print(fake_validity.shape)\n",
        "            break\n",
        "    \n",
        "    return gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"Discriminator\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 16,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 50,\n",
        "    \"epochs\": 1,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "discriminator = train_discriminator(args)"
      ],
      "metadata": {
        "id": "n5XD9r5u3rhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1432beab-0501-4aa6-a3e0-1837329203cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "File path: data/cifar-10-batches-py.tar.gz\n",
            "Transforming data...\n",
            "Beginning training ...\n",
            "torch.Size([50])\n",
            "torch.Size([50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM8DcScdiWnD"
      },
      "source": [
        "### Part (c) [3 pt]\n",
        "Modify the provided training code to implement a conditional GAN."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # train with real images\n",
        "    real_validity = discriminator(real_images, labels)\n",
        "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n",
        "    \n",
        "    # train with fake images\n",
        "    fake_images = generator(labels)\n",
        "\n",
        "    #\n",
        "    fake_validity = discriminator(fake_images, labels)\n",
        "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n",
        "\n",
        "    d_loss = real_loss + fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    # return d_loss.data[0]\n",
        "    return d_loss.item()\n",
        "\n",
        "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion, labels):\n",
        "    g_optimizer.zero_grad()\n",
        "    fake_images = generator(labels)\n",
        "    validity = discriminator(fake_images, labels)\n",
        "\n",
        "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    # return g_loss.data[0]\n",
        "    return g_loss.item()"
      ],
      "metadata": {
        "id": "Ne6rK4zhlGA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMPQPSykNTwi"
      },
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def get_torch_vars(xs, ys, gpu=False):\n",
        "    \"\"\"\n",
        "    Helper function to convert numpy arrays to pytorch tensors.\n",
        "    If GPU is used, move the tensors to GPU.\n",
        "\n",
        "    Args:\n",
        "      xs (float numpy tenosor): greyscale input\n",
        "      ys (int numpy tenosor): categorical labels\n",
        "      gpu (bool): whether to move pytorch tensor to GPU\n",
        "    Returns:\n",
        "      Variable(xs), Variable(ys)\n",
        "    \"\"\"\n",
        "    xs = torch.from_numpy(xs).float()\n",
        "    ys = torch.from_numpy(ys).float() #--> ADDED for cGAN\n",
        "    if gpu:\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "    return Variable(xs), Variable(ys)\n",
        "\n",
        "def train1(args, cnn=None):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # TODO: necessary?\n",
        "    torch.set_num_threads(5)\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE COLOURS CATEGORIES\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1 if not args.downsize_input else 3\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        Net = globals()[args.model]\n",
        "        cnn = Net(args.kernel, args.num_filters) # generator\n",
        "        discriminator = Discriminator(args.kernel, args.num_filters) # discriminator\n",
        "\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "\n",
        "    criterion = nn.BCELoss()                                                  \n",
        "    g_optimizer = torch.optim.Adam(cnn.parameters(), lr=1e-4)\n",
        "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "        discriminator.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()\n",
        "        discriminator.train()\n",
        "        losses = []\n",
        " \n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "\n",
        "            #--->ADDED 5\n",
        "            img_grey = images #greyscale\n",
        "            img_real = labels #real images\n",
        "            batch_size = args.batch_size\n",
        "            \n",
        "            #discriminator training\n",
        "            d_loss = discriminator_train_step(batch_size, discriminator,\n",
        "                                          cnn, d_optimizer, criterion,\n",
        "                                          img_real, img_grey)\n",
        "        \n",
        "\n",
        "            # generator training\n",
        "            g_loss = generator_train_step(batch_size, discriminator, cnn, g_optimizer, criterion, img_grey)\n",
        "\n",
        "            outputs=cnn(img_grey)\n",
        "\n",
        "        # print and visualize\n",
        "        print(epoch,g_loss, d_loss)\n",
        "        visual(images, labels, outputs, args.gpu, 1)\n",
        "\n",
        "    return cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KnN4Bc8iv_Y"
      },
      "source": [
        "### Part (d) [3 pt]\n",
        "Train a conditional GAN for image colourization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"Generator\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 50,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": False,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train1(args)"
      ],
      "metadata": {
        "id": "Un-jrOuONJbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ntCfgQkCAF"
      },
      "source": [
        "### Part (e) [1 pt]\n",
        "How does the performance of the cGAN compare with the autoencoder models that you tested in the first half of this assignment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stxojIJXkGvv"
      },
      "source": [
        "Answer:\n",
        "the overall images quality of autoencoders is higher than cGAN. the use of colors is more accurate on Autoendoers images than cGAN. Also, the images generated from cGAN show some consistent noise. Thus the performance of Autoencoder model is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2ez4CJdgQvP"
      },
      "source": [
        "### Part (f) [2 pt]\n",
        "\n",
        "A colour space is a choice of mapping of colours into three-dimensional coordinates. Some colours could be close together in one colour space, but further apart in others. The RGB colour space is probably the most familiar to you, the model used in in our regression colourization example computes squared error in RGB colour space. But, most state of the art colourization models\n",
        "do not use RGB colour space. How could using the RGB colour space be problematic? Your answer should relate how human perception of colour is different than the squared distance. You may use the Wikipedia article on colour space to help you answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5WXfVI7gQvQ"
      },
      "source": [
        "Answer:\n",
        "\n",
        "RGB is not good for image processing or storage.\n",
        "\n",
        "Besides, the human eye sees green best. This is partially because the human eye is most sensitive to luma, or how bright something is. Green is mixed in with brightness in human vision. Unfortunately, with RGB, brightness is mixed in to all the color channels, and there isn't an easy way to separate it out. Furthermore, human eyes may cannot identify specific color by using RGB since the difference between each color is nonlinear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jda8pXEQ4HKa"
      },
      "source": [
        "## Part 2. Exploration [10 pt]\n",
        "\n",
        "At this point we have trained a few different generative models for our image colourization task with varying results. What makes this work exciting is that there many other approaches we could take. In this part of the assignment you will be exploring at least one of several approaches towards improving our performance on the image colourization task. Some well known approaches you can consider include:\n",
        "\n",
        "- lab colour space representation instead of RBG which simplifies the problem and requires you to predict two output channels instead of three\n",
        "- k-means to represent RBG colourspace by 'k' distinct colours, this effectively changes the problem from regression to classification.\n",
        "\n",
        "Other interesting approaches include:\n",
        "- combining L1 loss along with the discriminator-based loss\n",
        "- starting with a pretrained generator (i.e. Resnet)\n",
        "- patch discriminator trained on local regions\n",
        "\n",
        "A great example of some of these different approaches can be found in a <a href=\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\">blog post by Moein Shariatnia</a>.\n",
        "\n",
        "Note you are only required to pick one of the suggested modifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT_h0FoSEa8F"
      },
      "source": [
        "from fastai.vision.learner import create_body\n",
        "from torchvision.models.resnet import resnet18\n",
        "from fastai.vision.models.unet import DynamicUnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai==2.4"
      ],
      "metadata": {
        "id": "akr3Ioiw1RLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_res_unet(n_input=1, n_output=3, size=32):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    body = create_body(resnet18, pretrained=True, n_in=n_input, cut=-2)\n",
        "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
        "    return net_G\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net_G = build_res_unet(n_input=1, n_output=3, size=32)\n"
      ],
      "metadata": {
        "id": "qJKM0Hvr0GTZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e312323f73714e2991f5d1eda8a4bc49",
            "222a6a98325b4d66a220ac642d7c6f1a",
            "cd394e007a184f558869435298a19dfc",
            "f73237dacacd44a08408667518307029",
            "84be4f0a34d04a1da70b0c25b4607293",
            "14f014f8f4624378b1af7142ff6f1963",
            "e9e1f1b7badb421380a792cd0b30c216",
            "cb2fd1a3d673428db9e8339401473b51",
            "61332e38ac1947b6894681863af54225",
            "985e6d4702e04bcc8e7dd67cbffd96cb",
            "3e9ffd1b52524f49bef80e94c4409d34"
          ]
        },
        "outputId": "a0a70b54-04e4-41c2-9aa7-fee10f915e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e312323f73714e2991f5d1eda8a4bc49"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train2(args, cnn):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # TODO: necessary?\n",
        "    torch.set_num_threads(5)\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE COLOURS CATEGORIES\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1 if not args.downsize_input else 3\n",
        "    # LOAD THE MODEL\n",
        "   \n",
        "    discriminator = Discriminator(args.kernel, args.num_filters) # discriminator\n",
        "\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "\n",
        "    criterion = nn.BCELoss()                                                  \n",
        "    g_optimizer = torch.optim.Adam(cnn.parameters(), lr=1e-4)\n",
        "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "        discriminator.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()\n",
        "        discriminator.train()\n",
        "        losses = []\n",
        " \n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "\n",
        "            #--->ADDED 5\n",
        "            img_grey = images #greyscale\n",
        "            img_real = labels #real images\n",
        "            batch_size = args.batch_size\n",
        "            \n",
        "            #discriminator training\n",
        "            d_loss = discriminator_train_step(batch_size, discriminator,\n",
        "                                          cnn, d_optimizer, criterion,\n",
        "                                          img_real, img_grey)\n",
        "        \n",
        "\n",
        "            # generator training\n",
        "            g_loss = generator_train_step(batch_size, discriminator, cnn, g_optimizer, criterion, img_grey)\n",
        "\n",
        "            outputs=cnn(img_grey)\n",
        "\n",
        "        # print and visualize\n",
        "        print(epoch,g_loss, d_loss)\n",
        "        visual(images, labels, outputs, args.gpu, 1)\n",
        "\n",
        "    return cnn"
      ],
      "metadata": {
        "id": "5wcGitcqIuOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"net_G\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 50,\n",
        "    \"epochs\": 20,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": False,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "cnn = train2(args, net_G)"
      ],
      "metadata": {
        "id": "uW1475QJHCho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W04U6MhOjezG"
      },
      "source": [
        "## Part 3. New Data [5 pt]\n",
        "Retrieve sample pictures from online and demonstrate how well your best model performs. Provide all your code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data retrived from internet.\n",
        "# choose cifar10 fifth class(deer) \n",
        "# choose the last 10000 images as our data\n",
        "\n",
        "DEER_CATEGORY = 4\n",
        "\n",
        "# convert colour images into greyscale\n",
        "def process(xs, ys, max_pixel=256.0, downsize_input=False):\n",
        "    \"\"\"\n",
        "    Pre-process CIFAR10 images by taking only the horse category,\n",
        "    shuffling, and have colour values be bound between 0 and 1\n",
        "\n",
        "    Args:\n",
        "      xs: the colour RGB pixel values\n",
        "      ys: the category labels\n",
        "      max_pixel: maximum pixel value in the original data\n",
        "    Returns:\n",
        "      xs: value normalized and shuffled colour images\n",
        "      grey: greyscale images, also normalized so values are between 0 and 1\n",
        "    \"\"\"\n",
        "    xs = xs / max_pixel\n",
        "    xs = xs[np.where(ys == DEER_CATEGORY)[0], :, :, :]\n",
        "    npr.shuffle(xs)\n",
        "\n",
        "    grey = np.mean(xs, axis=1, keepdims=True)\n",
        "\n",
        "    if downsize_input:\n",
        "        downsize_module = nn.Sequential(\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "        )\n",
        "        xs_downsized = downsize_module.forward(torch.from_numpy(xs).float())\n",
        "        xs_downsized = xs_downsized.data.numpy()\n",
        "        return (xs, xs_downsized)\n",
        "    else:\n",
        "        return (xs, grey)"
      ],
      "metadata": {
        "id": "ENg9IpCsd--N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vskeipFLjezJ"
      },
      "source": [
        "# Based on the model performance, the model using resnet as the generator has the best performance\n",
        "# choose it as our model.\n",
        "\n",
        "from fastai.vision.learner import create_body\n",
        "from torchvision.models.resnet import resnet18\n",
        "from fastai.vision.models.unet import DynamicUnet\n",
        "\n",
        "def build_res_unet(n_input=1, n_output=3, size=32):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    body = create_body(resnet18, pretrained=True, n_in=n_input, cut=-2)\n",
        "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
        "    return net_G\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net_G = build_res_unet(n_input=1, n_output=3, size=32)\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def get_torch_vars(xs, ys, gpu=False):\n",
        "    \"\"\"\n",
        "    Helper function to convert numpy arrays to pytorch tensors.\n",
        "    If GPU is used, move the tensors to GPU.\n",
        "\n",
        "    Args:\n",
        "      xs (float numpy tenosor): greyscale input\n",
        "      ys (int numpy tenosor): categorical labels\n",
        "      gpu (bool): whether to move pytorch tensor to GPU\n",
        "    Returns:\n",
        "      Variable(xs), Variable(ys)\n",
        "    \"\"\"\n",
        "    xs = torch.from_numpy(xs).float()\n",
        "    ys = torch.from_numpy(ys).float() #--> ADDED for cGAN\n",
        "    if gpu:\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "    return Variable(xs), Variable(ys)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        self.downconv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, num_filters, kernel_size=kernel,stride = 2, padding=padding), #32*16*16\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.LeakyReLU(0.2)) \n",
        "        \n",
        "        self.downconv2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, stride = 2, padding=padding), #64*8*8\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.LeakyReLU(0.2)) \n",
        "        \n",
        "        self.fc1 = nn.Linear(64*8*8, 15)\n",
        "        self.fc2 = nn.Linear(15, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "    \n",
        "    def forward(self, x, img_greyscale):\n",
        "        x = torch.cat([x, img_greyscale], 1)\n",
        "        conv1 = self.downconv1(x)\n",
        "        conv2 = self.downconv2(conv1)\n",
        "\n",
        "        out = conv2.view(-1, 64*8*8)\n",
        "\n",
        "        out = self.leakyrelu(self.fc1(out))\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "        out = out.squeeze()\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # train with real images\n",
        "    real_validity = discriminator(real_images, labels)\n",
        "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n",
        "    \n",
        "    # train with fake images\n",
        "    fake_images = generator(labels)\n",
        "\n",
        "    #\n",
        "    fake_validity = discriminator(fake_images, labels)\n",
        "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n",
        "\n",
        "    d_loss = real_loss + fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    # return d_loss.data[0]\n",
        "    return d_loss.item()\n",
        "\n",
        "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion, labels):\n",
        "    g_optimizer.zero_grad()\n",
        "    fake_images = generator(labels)\n",
        "    validity = discriminator(fake_images, labels)\n",
        "\n",
        "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    # return g_loss.data[0]\n",
        "    return g_loss.item()\n",
        "\n",
        "def train2(args, cnn):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # TODO: necessary?\n",
        "    torch.set_num_threads(5)\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # LOAD THE COLOURS CATEGORIES\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1 if not args.downsize_input else 3\n",
        "    # LOAD THE MODEL\n",
        "   \n",
        "    discriminator = Discriminator(args.kernel, args.num_filters) # discriminator\n",
        "\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "\n",
        "    criterion = nn.BCELoss()                                                  \n",
        "    g_optimizer = torch.optim.Adam(cnn.parameters(), lr=1e-4)\n",
        "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "        discriminator.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()\n",
        "        discriminator.train()\n",
        "        losses = []\n",
        " \n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "\n",
        "            #--->ADDED 5\n",
        "            img_grey = images #greyscale\n",
        "            img_real = labels #real images\n",
        "            batch_size = args.batch_size\n",
        "            \n",
        "            #discriminator training\n",
        "            d_loss = discriminator_train_step(batch_size, discriminator,\n",
        "                                          cnn, d_optimizer, criterion,\n",
        "                                          img_real, img_grey)\n",
        "        \n",
        "\n",
        "            # generator training\n",
        "            g_loss = generator_train_step(batch_size, discriminator, cnn, g_optimizer, criterion, img_grey)\n",
        "\n",
        "            outputs=cnn(img_grey)\n",
        "\n",
        "        # print and visualize\n",
        "        print(epoch,g_loss, d_loss)\n",
        "        visual(images, labels, outputs, args.gpu, 1)\n",
        "\n",
        "    return cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"net_G\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 50,\n",
        "    \"epochs\": 20,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": False,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "\n",
        "args.update(args_dict)\n",
        "cnn = train2(args, net_G)"
      ],
      "metadata": {
        "id": "-loRAllnd6SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYwI4RmFS2RB"
      },
      "source": [
        "### Saving to HTML\n",
        "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
        "\n",
        "(1) download your ipynb file by clicking on File->Download.ipynb\n",
        "\n",
        "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
        "\n",
        "(3) run the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TrsqdNgS5ex"
      },
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html LAB_3_Generating_Data.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuXhlFlPTY7F"
      },
      "source": [
        "(4) the html file will be available for download in the temporary Google Colab storage\n",
        "\n",
        "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
      ]
    }
  ]
}